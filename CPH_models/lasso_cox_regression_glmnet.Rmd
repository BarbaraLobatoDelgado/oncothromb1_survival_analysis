---
title: "Lasso Cox Regression"
author: "Bárbara Lobato Delgado"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Settings and load libraries

```{r}
# Deactivate scientific notation
options(scipen = 999, max.print = 10000)

# Seed
seed = 2828
set.seed(seed)
```


```{r}
find_column_mismatch <- function(df1, df2) {
  # Find column mismatch
  missing_in_valid <- setdiff(colnames(df1), colnames(df2))
  extra_in_valid <- setdiff(colnames(df2), colnames(df1))
  
  if (length(missing_in_valid) > 0 || length(extra_in_valid) > 0) {
    msg <- c()
    if (length(missing_in_valid) > 0) {
      msg <- c(msg, sprintf("Columns missing in validation: %s", 
                            paste(missing_in_valid, collapse = ", ")))
    }
    if (length(extra_in_valid) > 0) {
      msg <- c(msg, sprintf("Extra columns in validation: %s",
                          paste(extra_in_valid, collapse = ", ")))
    }
    stop(paste(msg, collapse = "; "), call. = FALSE)
  }
}
```


```{r}
# Load libraries
pacman::p_load(
  an9elproject, 
  tidyverse,
  lubridate, 
  magrittr,
  fastDummies,
  glmnet,
  survival,
  survminer, 
  penalized,
  powerSurvEpi,
  car,
  StepReg,
  plotly,
  install = FALSE, update = FALSE
  )
```

## Load data

```{r cars}
# # Load time-dependent dataset
# oncoth1_td_dataset = read.csv(file = "/mnt/ir-bioinf02/home/blobato/oncothromb01/data/processed/oncoth1_td_dataset.csv")
```


```{r}
# Load an9elproject DB
oncoth1 <- get_project("oncothr1", version = "0.0.8005") 
```


```{r}
# Get data slot
oncoth1_data <- oncoth1$data
```


```{r}
# Get meaningful variables
oncoth1_survival_df <- oncoth1_data %>%
  filter(patient_inc_exc == "Included") %>%
  select(id, 
         patient_group,
         # Follow-up data
         censored_patient, # zero for censored patients (alive at the time point), one for dead
         follow_up_length_from_cancer_diagnosis_days, 
         # Covariates
         age_when_cancer_dx, 
         gender, 
         bmi_value, 
         performance_status_category_corrected_imp, 
         diabetes_mellitus,
         dyslipidemia, 
         arterial_hypertension, 
         tobacco_use_imp, 
         copd_imp, 
         venous_insufficiency_imp, 
         khorana_risk_score_imp, 
         two_groups_krs,
         previous_onco_surgery,
         tumor_surgically_removed, 
         primary_tumor_simplified, 
         tnm_stage,
         tnm_stage_grouped, 
         tnm_stage_two_groups, 
         histology_type_imp,
         mucinous_histology_imp,
         grade_histological_differentiation_imp,
         catheter_device_imp, 
         family_background_vte, 
         previous_vte,  
         previous_ate, 
         anticoag_tx_currently, 
         antiaggreg_tx_currently
         ) %>%
  # Create VTE variable
  mutate(VTE = ifelse(patient_group == "Case", 1, 0)) %>%
  select(-patient_group) %>%
  # Create dataset for survival analysis
  rename(death = censored_patient, 
         time = follow_up_length_from_cancer_diagnosis_days)
  
  
```


```{r}
# Stop if there are missing values in dataset
stopifnot(!anyNA(oncoth1_survival_df))
```


```{r}
cols2select <- c(
 "VTE",
 "age_when_cancer_dx", 
 "gender", 
 "bmi_value", 
 "performance_status_category_corrected_imp", 
 # "diabetes_mellitus",
 "dyslipidemia", 
 "arterial_hypertension", 
 # "tobacco_use_imp", 
 # "copd_imp", 
 # "venous_insufficiency_imp", 
 # "khorana_risk_score_imp",# correlated to a lot of features
 # "two_groups_krs", # correlated to a lot of features
 # "previous_onco_surgery",
 "tumor_surgically_removed", 
 "primary_tumor_simplified", 
 # "tnm_stage",
 # "tnm_stage_grouped",
 "tnm_stage_two_groups",
 # "histology_type_imp", # unequal categories between train and validation set
 "mucinous_histology_imp",
 # "grade_histological_differentiation_imp",
 # "catheter_device_imp", 
 # "family_background_vte",
 "previous_vte",
 "previous_ate", 
 "anticoag_tx_currently", # unequal categories between train and validation set
 "antiaggreg_tx_currently"
 )
```


```{r}
  # vte_event +
  # age_when_cancer_dx +
  # gender +
  # bmi_value +
  # performance_status_category_corrected_imp + # Multicollinearity with cancer type, tobacco use, tumor resection in some folds, but models can be trained
  # # diabetes_mellitus + # Confounder. We kfoldsnow it is associated with pancreatic cancer patients that undergo resection of pancreas
  # dyslipidemia +
  # # tobacco_use_imp + # Correlated to NSCLC. Multicollinearity if PS is also selected
  # # copd_imp + # Correlated to tobacco use
  # arterial_hypertension +
  # # venous_insufficiency_imp +  # remove as suggested by Andrés Muñoz (bad definition of variable, information may refer to different conditions)
  # # kfoldshorana_riskfolds_score_imp + # Correlated to pancreatic and esophago-gastric cancers
  # # two_groups_kfoldsrs + # Correlated to pancreatic and esophago-gastric cancers
  # # previous_onco_surgery + # Correlated to having had tumor resected
  # tumor_surgically_removed +
  # primary_tumor_simplified +
  # # tnm_stage_grouped +
  # tnm_stage_two_groups +
  # # histology_type_imp + # Correlated with NSCLC
  # mucinous_histology_imp +
  # # grade_histological_differentiation_imp + # violates PH assumption
  # # catheter_device_imp + # quite correlated with tumor type, as Cramer's V indicates
  # # family_backfoldsground_vte + # High GVIF, multicollinearity with venous insufficiency in some folds. When selected, not clear if it is riskfolds or protective factor, due to lackfolds of information when recruiting patients
  # previous_vte +
  # previous_ate +
  # anticoag_tx_currently +
  # antiaggreg_tx_currently,
```


```{r}
process_data <- function(data, cols2select, apply_scaling = FALSE) {
  
  missing_cols <- setdiff(cols2select, colnames(data))
  if (length(missing_cols) > 0) {
    stop(paste("The following columns are missing from the data:", 
               paste(missing_cols, collapse = ", ")))
  }
  
  # Select columns as dependent variables
  data_selection <- data[, cols2select]
  
  # Convert yes/no columns into 1/0
  data_selection <- data_selection %>%
    mutate(
      across(
        .cols = where(~ (is.character(.) || is.factor(.)) && 
                        all(as.character(.) %in% c("Yes", "No"))), 
        .fns = ~ as.integer(as.character(.) == "Yes")
      )
    )
  
  # Transform other columns to numerical
  if ("grade_histological_differentiation_imp" %in% colnames(data_selection)){
    data_selection <- data_selection %>%
      mutate(grade_histological_differentiation_imp = as.numeric(case_when(
        str_starts(grade_histological_differentiation_imp, "Well") ~ 1,
        str_starts(grade_histological_differentiation_imp, "Moderately") ~ 2,
        str_starts(grade_histological_differentiation_imp, "Poorly") ~ 3, 
        TRUE ~ NA_real_
    )))
  }
   
  # Convert categorical to numeric
  data_dummies <- dummy_cols(
    .data = data_selection, 
    select_columns = NULL, 
    remove_first_dummy = TRUE, 
    remove_selected_columns = TRUE
    )
  
  # Transform covariates dataframe into matrix
  covariates <- as.matrix(data_dummies)
  
  # Scale 
  if (apply_scaling == TRUE) {
    covariates <- scale(covariates)
  }
  
  return(covariates)
}
```


```{r}
oncoth1_transf <- process_data(oncoth1_survival_df, cols2select = cols2select)
```


```{r}
Y0 <- with(oncoth1_survival_df, Surv(time, death))
Y0
```


## Inspect correlation

```{r}
# Calculate correlation
corr_covariates <- oncoth1_transf %>% cor(.)
```


```{r}
# Interactive heatmap
plot_ly(
  data = reshape2::melt(corr_covariates),
  x = ~Var1,
  y = ~Var2,
  z = ~value,
  type = "heatmap",
  colors = colorRamp(c("blue", "white", "red")),
  colorbar = list(title = "Correlation"),
  showscale = TRUE
  ) %>%
  layout(
    title = "Interactive Heatmap of Correlations in ONCOTHROMB1",
    xaxis = list(title = "Variables"),
    yaxis = list(title = "Variables")
  )
```


## Dataset partition

```{r}
# Partition data with repeated rows per patient into training and test set
ids_row_map <- data.frame(
  "row" = row.names(oncoth1_survival_df), 
  "id" = oncoth1_survival_df$id
  )

# Get unique IDs
unique_ids <- unique(oncoth1_survival_df$id)

# Randomly assign each unique ID to either train or test
train_ids <- sample(unique_ids, size = floor(0.8 * length(unique_ids))) # 80% for training
test_ids <- setdiff(unique_ids, train_ids)  # Remaining IDs for testing

# No match between train and test IDs
stopifnot(train_ids != test_ids)

# Get rows of train and test patients
train_rows <- which(oncoth1_survival_df$id %in% train_ids)
test_rows <- which(oncoth1_survival_df$id %in% test_ids)

# No match between train and test rows
stopifnot(train_rows != test_rows)
```


```{r}
# Save IDs in training and test split
write.csv(x = train_ids, file = "../partition_indices/glmnet_train_ids.csv", row.names = FALSE)
write.csv(x = test_ids, file = "../partition_indices/glmnet_test_ids.csv", row.names = FALSE)

# Save row number in training and test split
write.csv(x = train_rows, file = "../partition_indices/glmnet_train_rows.csv", row.names = FALSE)
write.csv(x = test_rows, file = "../partition_indices/glmnet_test_rows.csv", row.names = FALSE)
```


```{r}
# Create target variable
y <- with(oncoth1_survival_df, Surv(time, death))
y
```


```{r}
# Partition data using rows
training_set <- oncoth1_survival_df[train_rows,]
test_set <- oncoth1_survival_df[test_rows,]

# Partition target
y_train <- y[train_rows]
y_test <- y[test_rows]

# Check that all patients are accounted for in training and test set
stopifnot(
  nrow(training_set) + nrow(test_set) == nrow(oncoth1_transf),
  length(y_train) + length(y_test) == length(y)
)
```


```{r}
# Number of folds
kfolds <- 10
```


```{r}
# Create a fold-map at the patient level
shuffled_ids <- sample(unique_ids)
fold_sizes <- floor(length(shuffled_ids) / kfolds)
remainder <- length(shuffled_ids) %% kfolds

fold_vec <- rep(
  1:kfolds, 
  times = c(
    rep(fold_sizes + 1, remainder), 
    rep(fold_sizes, kfolds - remainder)
  )
)
# Tibble with IDs and folds mapped
fold_map <- tibble(id = shuffled_ids, fold = fold_vec)

# Join to have row number
cv_info_mapping <- fold_map %>%
  left_join(ids_row_map, by = "id") %>%
  mutate(set = case_when(
    id %in% train_ids ~ "train",
    id %in% test_ids ~ "test"
    )
    )
```


```{r}

```



```{r}
# Cross-validation loop
set.seed(seed)

# zero is ridge, 1 is lasso, in between is elastic net

# Define lambda grid
# alpha_grid <- c(0, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.5, 1) 
# Lasso
alpha_grid <- 1
# Lasso regularization strenght
lambda_grid <- c(0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1)

# Store regularization pathway results
# Do combinations of all folds and alpha values
cv_out <- expand.grid(
  alpha = alpha_grid, # alpha_grid, 
  lambda = lambda_grid,
  fold = 1:kfolds, 
  stringsAsFactors = FALSE
  ) %>%
  as_tibble() %>%
  mutate(
    cindex = NA_real_, 
    selected_vars = vector("list", n())
  )

# 
# vif_results <- vector("list", length = k)

# Loop over each row of that tibble
for (i in seq_len(nrow(cv_out))) {
  # Get alpha and fold from tibble with combinations
  i_alpha <- cv_out$alpha[i]
  i_lambda <- cv_out$lambda[i]
  i_fold <- cv_out$fold[i]
  
  # Get validation row number those not in this fold
  valid_ids <- cv_info_mapping %>% 
    filter(set == "train", fold == i_fold) %>% 
    pull(id) %>%
    as.integer()
  # Get train row number
  tr_ids <- cv_info_mapping %>% 
    filter(set == "train", fold != i_fold) %>% 
    pull(id) %>%
    as.integer()
  
  # Sanity check
  stopifnot(!valid_ids %in% tr_ids)
  
  # Subset data
  valid_data <- training_set %>% filter(id %in% valid_ids)
  tr_data <- training_set %>% filter(id %in% train_ids)
  
  # Get dependent variable (Surv object)
  y_valid <- with(valid_data, Surv(time, death))
  y_train <- with(tr_data, Surv(time, death))
  
  # Sanity check
  stopifnot(
    all(tr_data$id %in% train_ids), 
    all(valid_data$id %in% valid_ids)
  )
  
  # Select covariates
  valid_data_subset <- process_data(valid_data, cols2select)
  tr_data_subset <- process_data(tr_data, cols2select)
  
  # Learn scaling parameters on train data
  preproc <- caret::preProcess(
    x = tr_data_subset,
    method = c("center", "scale")
  )
  # Scale data
  X_train <- predict(preproc, tr_data_subset)
  X_valid <- predict(preproc, valid_data_subset)
  
  #--------------------------
  cvfit <- cv.glmnet(
    x = X_train,
    y = y_train, 
    family = "cox", 
    alpha = 1, 
    nfolds = 10
  )
  lambda_min <- cvfit$lambda.min     # λ that minimizes CV error
  lambda_1se <- cvfit$lambda.1se     # most regularized λ within 1 SE
  
  # 4a) Coefficients at lambda.min
  coefs_min <- coef(cvfit, s = "lambda.min")
  selected_min <- rownames(coefs_min)[as.numeric(coefs_min) != 0]
  
  # 4b) Coefficients at lambda.1se
  coefs_1se <- coef(cvfit, s = "lambda.1se")
  selected_1se <- rownames(coefs_1se)[as.numeric(coefs_1se) != 0]
  
  cat("At lambda.min:", lambda_min, "selected:", selected_min, "\n")
  cat("At lambda.1se:", lambda_1se, "selected:", selected_1se, "\n")
  
  # On training data
  lp_train <- predict(cvfit, newx = X_train, s = "lambda.min", type = "link")
  cidx_train <- concordance(y_train ~ as.numeric(lp_train))$concordance
  
  # On held-out test data
  # X_test <- as.matrix(test_df[, cols2select])
  # Y_test <- with(test_df, Surv(time, event))
  
  lp_test <- predict(cvfit, newx = X_valid, s = "lambda.min", type = "link")
  cidx_test <- concordance(y_valid ~ as.numeric(lp_test))$concordance
  
  cat("Train C-index:", round(cidx_train,3), 
      " Valid C-index:", round(cidx_test,3), "\n")
  #--------------------------
  
  
  
  
  # Train lasso Cox model
  lasso_cox <- glmnet(
    x = X_train, 
    y = y_train,
    family = "cox", 
    alpha = i_alpha, 
    lambda = i_lambda
  )
  
  coefs <- coef(lasso_cox, s = alpha)
  
  # Evaluate using C-index
  linear_pred <- as.numeric(predict(
    lasso_cox, 
    newx = X_valid, 
    s = alpha, 
    type = "link"
    ))
  
  # Calculate C-index and add to results
  cv_out$cindex[i] <- survival::concordance(y_valid ~ linear_pred)$concordance
  
  # Record selected variables
  cv_out$selected_vars[[i]] <- coef(lasso_cox, s = alpha)
  
}
```


## Regularization pathway

```{r}
# Summarize mean C-index by alpha
summary_regpath <- cv_out %>%
  group_by(lambda) %>%
  summarise(
    mean_cindex = mean(cindex, na.rm = TRUE), 
    .groups = "drop"
  )
summary_regpath
```


```{r}
# ggplot(summary_regpath, aes(x = lambda, y = mean_cindex)) + 
#   geom_line(size = 1) + 
#   geom_point(size = 2) + 
#   scale_x_log10(
#     breaks = unique(summary_regpath$lambda), 
#     labels = scales::scientific_format(digits = 2)
#   ) + 
#   labs(
#     x = "Lambda (lasso regularization parameter)", 
#     y = "Mean CV C-index", 
#     title = "Regularization path: mean C-index vs penalty strength"
#   ) + 
#   theme_minimal() + 
#   theme(
#     plot.title = element_text(hjust = 0.5), 
#     axis.text.x = element_text(angle = 45, hjust = 1)
#   )
```

## More frequently selected variables

```{r}
freq_selected_vars <- cv_out %>%
  mutate(
    selected = map(
      selected_vars, 
      # Extract rownames for non-zero coefficients
      ~ {
        nonzero_idx <- which(.x != 0)
        if (length(nonzero_idx) == 0) {
          character(0)
        } else {
          rownames(.x)[nonzero_idx]
        }
      }
    )
  ) %>%
  # Keep lambda/fold id and list of selected variables
  select(alpha, fold, selected) %>%
  # Explode the list-column to have lambda, fold and vars in one row
  unnest(selected) %>%
  # Count how many times each variable appears
  count(variable = selected, name = "freq") %>%
  mutate(rel_freq = freq / nrow(cv_out)) %>%
  arrange(desc(rel_freq))

freq_selected_vars
```



```{r}
# Then, train final model with selected features and compute C-index on test set
# ...
```






